
from langchain_community.embeddings.ollama import OllamaEmbeddings
from database_controller import DatabaseController
from query_controller import QueryController
from langchain_chroma import Chroma
import streamlit as st

#=============================================================================#

LLM_MODEL       = "gemma2:2b"
EMBEDDING_MODEL = "all-minilm"

QUERY_NUM   = 5
CHROMA_PATH = "chroma"

#=============================================================================#

# åˆå§‹åŒ–Chromaå‘é‡å­˜å„²
database = Chroma(
    persist_directory  = CHROMA_PATH, 
    embedding_function = OllamaEmbeddings(model=EMBEDDING_MODEL)
    )

QueryController    = QueryController(database, LLM_MODEL, QUERY_NUM)
DatabaseController = DatabaseController(database)

#=============================================================================#

st.set_page_config(layout="wide")

if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "system", "content": "ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”å•é¡Œ", "source": None}]

    if len(DatabaseController.calculate_existing_ids()) == 0:
        info = "ğŸ‘ˆ Hi~ è³‡æ–™åº«æ˜¯ç©ºçš„ï¼Œè«‹å…ˆåˆ°Dataé é¢é»é¸ä¸Šå‚³è³‡æ–™ã€‚"
        st.session_state.messages.append({"role": "assistant", "content": info, "source": None})

    else:
        info = "âœ‹ Hi~ è«‹å•æƒ³è©¢å•ä»€éº¼å•é¡Œå‘¢ï¼Ÿ"
        st.session_state.messages.append({"role": "assistant", "content": info, "source": None})

#=============================================================================#

st.title("è³‡æ–™æŸ¥è©¢")

#-----------------------------------------------------------------------------#

for message in st.session_state.messages[1:]:

    if message["role"] == "user":
        with st.chat_message("user", avatar="ğŸ¦–"):
            st.markdown(message["content"])

    else:
        with st.chat_message("assistant", avatar="ğŸ¤–"):
            st.markdown(message["content"])

            if message["source"] is not None:
                st.caption(message["source"])

#-----------------------------------------------------------------------------#

if question := st.chat_input("è¼¸å…¥å•é¡Œ"):

    with st.chat_message("user", avatar="ğŸ¦–"):
        st.markdown(question)

#-----------------------------------------------------------------------------#

    results, sources = QueryController.generate_results(question)

    prompt = QueryController.generate_prompt(question, results)
    
    source_info = "è³‡æ–™ä¾†æº: " + ", ".join(sources)

    st.session_state.messages.append({"role": "user", "content": prompt, "source": None})

#-----------------------------------------------------------------------------#

    with st.chat_message("assistant", avatar="ğŸ¤–"):

        response = st.write_stream(QueryController.ollama_generator(st.session_state.messages))

    st.caption(source_info)

    st.session_state.messages[-1]["content"] = question

    st.session_state.messages.append({"role": "assistant", "content": response, "source": source_info})
